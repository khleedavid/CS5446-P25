{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dependencies for the project\n",
    "import gymnasium as gym\n",
    "import gym_BinPack3D\n",
    "from gym_BinPack3D.envs import Box, Rotate\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#register the environment\n",
    "gym.envs.register(\n",
    "    id='BinPack3D-v1',\n",
    "    entry_point='gym_BinPack3D.envs:PackingGame',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the environment.\n",
    "#container_size: size of the container in 3D\n",
    "#boxSeqGenerator: how the boxes are generated.\n",
    "#enabled_rotations: which rotations are allowed for the boxes\n",
    "#n_foreseeable_box: how many boxes are shown to the agent\n",
    "#box_set: the set of boxes that are used in the environment. \n",
    "\n",
    "DATA_DIR = os.path.join(os.getcwd(), 'data')\n",
    "LOG_DIR = os.path.join(os.getcwd(), 'log')\n",
    "EVAL_LOG_DIR = os.path.join(os.getcwd(), 'eval_log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy, plot_results\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "\n",
    "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
    "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
    "\n",
    "    :param check_freq:\n",
    "    :param log_dir: Path to the folder where the model will be saved.\n",
    "      It must contains the file created by the ``Monitor`` wrapper.\n",
    "    :param verbose: Verbosity level: 0 for no output, 1 for info messages, 2 for debug messages\n",
    "    \"\"\"\n",
    "    def __init__(self, check_freq: int, log_dir: str, verbose: int = 1):\n",
    "        super().__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.log_dir = log_dir\n",
    "        self.save_path = os.path.join(log_dir, \"best_model\")\n",
    "        self.best_mean_reward = -np.inf\n",
    "\n",
    "    def _init_callback(self) -> None:\n",
    "        # Create folder if needed\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "\n",
    "          # Retrieve training reward\n",
    "          x, y = ts2xy(load_results(self.log_dir), \"timesteps\")\n",
    "          if len(x) > 0:\n",
    "              # Mean training reward over the last 100 episodes\n",
    "              mean_reward = np.mean(y[-100:])\n",
    "              if self.verbose >= 1:\n",
    "                print(f\"Num timesteps: {self.num_timesteps}\")\n",
    "                print(f\"Best mean reward: {self.best_mean_reward:.2f} - Last mean reward per episode: {mean_reward:.2f}\")\n",
    "\n",
    "              # New best model, you could save the agent here\n",
    "              if mean_reward > self.best_mean_reward:\n",
    "                  self.best_mean_reward = mean_reward\n",
    "                  # Example for saving best model\n",
    "                  if self.verbose >= 1:\n",
    "                    print(f\"Saving new best model\")\n",
    "                  self.model.save(self.save_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "\n",
    "#run prediction of given @model and store it in a gif animation\n",
    "def create_gif(model, vec_env, path, fps=5):\n",
    "    frames = []\n",
    "    obs = model.env.reset()\n",
    "    frame = model.env.render(mode=\"rgb_array\")\n",
    "\n",
    "    for i in range(100):\n",
    "        frames.append(frame)\n",
    "        action, _state = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, info = vec_env.step(action)\n",
    "        frame = model.env.render(mode=\"rgb_array\")\n",
    "        if frame is None:\n",
    "            print(\"Frame is None!!\")\n",
    "            break\n",
    "\n",
    "    imageio.mimsave(path, frames, fps=fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using fixed box sequence\n",
      "Box to be sampled:\n",
      "Box: Size 2 1 1 Position 0 0 0\n",
      "Box: Size 1 2 1 Position 0 0 0\n",
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ekabu\\anaconda3\\lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:135: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting numpy array dtype to be float32, actual type: int32\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\ekabu\\anaconda3\\lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:159: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "c:\\Users\\ekabu\\anaconda3\\lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:135: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting numpy array dtype to be float32, actual type: int32\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\ekabu\\anaconda3\\lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:159: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=1000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 2.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2        |\n",
      "|    mean_reward     | -100     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 2.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2        |\n",
      "|    mean_reward     | -100     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2        |\n",
      "|    ep_rew_mean     | -100     |\n",
      "| time/              |          |\n",
      "|    fps             | 179      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 11       |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 2.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 2        |\n",
      "|    mean_reward          | -100     |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 3000     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.0      |\n",
      "|    clip_fraction        | 0        |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | 0        |\n",
      "|    explained_variance   | 0.0809   |\n",
      "|    learning_rate        | 0.0003   |\n",
      "|    loss                 | 3.63e+03 |\n",
      "|    n_updates            | 10       |\n",
      "|    policy_gradient_loss | -6.7e-07 |\n",
      "|    value_loss           | 8.13e+03 |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 2.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2        |\n",
      "|    mean_reward     | -100     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2        |\n",
      "|    ep_rew_mean     | -100     |\n",
      "| time/              |          |\n",
      "|    fps             | 192      |\n",
      "|    iterations      | 2        |\n",
      "|    time_elapsed    | 21       |\n",
      "|    total_timesteps | 4096     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 2.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 2        |\n",
      "|    mean_reward          | -100     |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 5000     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.0      |\n",
      "|    clip_fraction        | 0        |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | 0        |\n",
      "|    explained_variance   | 0.000293 |\n",
      "|    learning_rate        | 0.0003   |\n",
      "|    loss                 | 3.12e+03 |\n",
      "|    n_updates            | 20       |\n",
      "|    policy_gradient_loss | 2.29e-07 |\n",
      "|    value_loss           | 6.79e+03 |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 2.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2        |\n",
      "|    mean_reward     | -100     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2        |\n",
      "|    ep_rew_mean     | -100     |\n",
      "| time/              |          |\n",
      "|    fps             | 196      |\n",
      "|    iterations      | 3        |\n",
      "|    time_elapsed    | 31       |\n",
      "|    total_timesteps | 6144     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 2.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 2         |\n",
      "|    mean_reward          | -100      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 7000      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | 0         |\n",
      "|    explained_variance   | 0.00014   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.67e+03  |\n",
      "|    n_updates            | 30        |\n",
      "|    policy_gradient_loss | -2.44e-08 |\n",
      "|    value_loss           | 5.78e+03  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 2.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2        |\n",
      "|    mean_reward     | -100     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2        |\n",
      "|    ep_rew_mean     | -100     |\n",
      "| time/              |          |\n",
      "|    fps             | 199      |\n",
      "|    iterations      | 4        |\n",
      "|    time_elapsed    | 41       |\n",
      "|    total_timesteps | 8192     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 2.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 2         |\n",
      "|    mean_reward          | -100      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 9000      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | 0         |\n",
      "|    explained_variance   | 0.000113  |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.24e+03  |\n",
      "|    n_updates            | 40        |\n",
      "|    policy_gradient_loss | -5.76e-07 |\n",
      "|    value_loss           | 4.88e+03  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 2.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2        |\n",
      "|    mean_reward     | -100     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2        |\n",
      "|    ep_rew_mean     | -100     |\n",
      "| time/              |          |\n",
      "|    fps             | 201      |\n",
      "|    iterations      | 5        |\n",
      "|    time_elapsed    | 50       |\n",
      "|    total_timesteps | 10240    |\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "#train with ppo\n",
    "# import gymnasium as gym\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy, plot_results\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "\n",
    "# Create log dirs if not yet exist\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "os.makedirs(EVAL_LOG_DIR, exist_ok=True)\n",
    "\n",
    "env = gym.make('BinPack3D-v1', \n",
    "                container_size = (8, 4, 4),\n",
    "                boxSeqGenerator='fixed', \n",
    "                #enabled_rotations = [Rotate.NOOP, Rotate.XY, Rotate.XZ, Rotate.YZ],\n",
    "                enabled_rotations = [Rotate.NOOP],\n",
    "                n_foreseeable_box = 3,\n",
    "                box_set = [Box(2,1,1), Box(1,2,1)]\n",
    "            )\n",
    "\n",
    "env = Monitor(env, LOG_DIR)\n",
    "\n",
    "vec_env = DummyVecEnv([lambda: env])\n",
    "model = PPO(\"MultiInputPolicy\", vec_env, verbose=1)\n",
    "\n",
    "# Create the callback: check every 5000 steps\n",
    "callback = SaveOnBestTrainingRewardCallback(check_freq=5000, log_dir=LOG_DIR)\n",
    "\n",
    "# Create eval callback that evaluates agent for 5 episodes every 500 training environment steps.\n",
    "eval_callback = EvalCallback(env, best_model_save_path=EVAL_LOG_DIR,\n",
    "                              log_path=EVAL_LOG_DIR, eval_freq=1000,\n",
    "                              deterministic=True,\n",
    "                              render=False)\n",
    "\n",
    "timesteps = 10_000\n",
    "model.learn(total_timesteps=timesteps, callback=eval_callback)\n",
    "\n",
    "#plot training process (uncomment below to see training for each timestamp)\n",
    "#plot_results([LOG_DIR], timesteps, results_plotter.X_TIMESTEPS, \"AirCargo\")\n",
    "#plt.show()\n",
    "\n",
    "#save the model\n",
    "model.save(DATA_DIR+\"/ppo_model\")\n",
    "\n",
    "#laod model and create gif\n",
    "PPO_GIF_DIR = DATA_DIR+\"/ppo.gif\"\n",
    "ppo_model = PPO.load(DATA_DIR+\"/ppo_model\", env=vec_env)\n",
    "create_gif(ppo_model, vec_env, PPO_GIF_DIR, fps=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using fixed box sequence\n",
      "Box to be sampled:\n",
      "Box: Size 2 1 1 Position 0 0 0\n",
      "Box: Size 2 1 1 Position 0 0 0\n",
      "Box: Size 2 1 1 Position 0 0 0\n",
      "Box: Size 1 2 1 Position 0 0 0\n",
      "Box: Size 1 2 1 Position 0 0 0\n",
      "Using cuda device\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 199      |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 500      |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0       |\n",
      "|    explained_variance | -0.00945 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | -0       |\n",
      "|    value_loss         | 7.04e+03 |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 2.40 +/- 0.49\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.4      |\n",
      "|    mean_reward        | -100     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0       |\n",
      "|    explained_variance | -0.00296 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | -0       |\n",
      "|    value_loss         | 6.18e+03 |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 198  |\n",
      "|    iterations      | 200  |\n",
      "|    time_elapsed    | 5    |\n",
      "|    total_timesteps | 1000 |\n",
      "-----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ekabu\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 199      |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0       |\n",
      "|    explained_variance | -0.00134 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | -0       |\n",
      "|    value_loss         | 5.45e+03 |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 2.80 +/- 0.75\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.8       |\n",
      "|    mean_reward        | -100      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0        |\n",
      "|    explained_variance | -0.000694 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | -0        |\n",
      "|    value_loss         | 4.79e+03  |\n",
      "-------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 197  |\n",
      "|    iterations      | 400  |\n",
      "|    time_elapsed    | 10   |\n",
      "|    total_timesteps | 2000 |\n",
      "-----------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 198       |\n",
      "|    iterations         | 500       |\n",
      "|    time_elapsed       | 12        |\n",
      "|    total_timesteps    | 2500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0        |\n",
      "|    explained_variance | -0.000392 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 499       |\n",
      "|    policy_loss        | -0        |\n",
      "|    value_loss         | 4.18e+03  |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 2.80 +/- 0.75\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.8       |\n",
      "|    mean_reward        | -100      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0        |\n",
      "|    explained_variance | -0.000224 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | -0        |\n",
      "|    value_loss         | 3.61e+03  |\n",
      "-------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 196  |\n",
      "|    iterations      | 600  |\n",
      "|    time_elapsed    | 15   |\n",
      "|    total_timesteps | 3000 |\n",
      "-----------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 196       |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 17        |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0        |\n",
      "|    explained_variance | -0.000138 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | -0        |\n",
      "|    value_loss         | 3.08e+03  |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 2.80 +/- 0.75\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.8       |\n",
      "|    mean_reward        | -100      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0        |\n",
      "|    explained_variance | -8.59e-05 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | -0        |\n",
      "|    value_loss         | 2.6e+03   |\n",
      "-------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 195  |\n",
      "|    iterations      | 800  |\n",
      "|    time_elapsed    | 20   |\n",
      "|    total_timesteps | 4000 |\n",
      "-----------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 194       |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 23        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0        |\n",
      "|    explained_variance | -5.66e-05 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | -0        |\n",
      "|    value_loss         | 2.16e+03  |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 2.80 +/- 0.75\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.8       |\n",
      "|    mean_reward        | -100      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0        |\n",
      "|    explained_variance | -4.04e-05 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | -0        |\n",
      "|    value_loss         | 1.75e+03  |\n",
      "-------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 193  |\n",
      "|    iterations      | 1000 |\n",
      "|    time_elapsed    | 25   |\n",
      "|    total_timesteps | 5000 |\n",
      "-----------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 192       |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 28        |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0        |\n",
      "|    explained_variance | -2.94e-05 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | -0        |\n",
      "|    value_loss         | 1.39e+03  |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 2.80 +/- 0.75\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.8       |\n",
      "|    mean_reward        | -100      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0        |\n",
      "|    explained_variance | -1.86e-05 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | -0        |\n",
      "|    value_loss         | 1.08e+03  |\n",
      "-------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 191  |\n",
      "|    iterations      | 1200 |\n",
      "|    time_elapsed    | 31   |\n",
      "|    total_timesteps | 6000 |\n",
      "-----------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 191      |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 33       |\n",
      "|    total_timesteps    | 6500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0       |\n",
      "|    explained_variance | -2.5e-05 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | -0       |\n",
      "|    value_loss         | 798      |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 2.80 +/- 0.75\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.8       |\n",
      "|    mean_reward        | -100      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0        |\n",
      "|    explained_variance | -1.42e-05 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | -0        |\n",
      "|    value_loss         | 561       |\n",
      "-------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 191  |\n",
      "|    iterations      | 1400 |\n",
      "|    time_elapsed    | 36   |\n",
      "|    total_timesteps | 7000 |\n",
      "-----------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 190       |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 39        |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0        |\n",
      "|    explained_variance | -5.48e-06 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | -0        |\n",
      "|    value_loss         | 367       |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 2.80 +/- 0.75\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.8       |\n",
      "|    mean_reward        | -100      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0        |\n",
      "|    explained_variance | -1.53e-05 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | -0        |\n",
      "|    value_loss         | 213       |\n",
      "-------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 190  |\n",
      "|    iterations      | 1600 |\n",
      "|    time_elapsed    | 41   |\n",
      "|    total_timesteps | 8000 |\n",
      "-----------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 190      |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 44       |\n",
      "|    total_timesteps    | 8500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0       |\n",
      "|    explained_variance | -3.1e-06 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | -0       |\n",
      "|    value_loss         | 101      |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 2.80 +/- 0.75\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 2.8       |\n",
      "|    mean_reward        | -100      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0        |\n",
      "|    explained_variance | -1.54e-05 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | -0        |\n",
      "|    value_loss         | 30.6      |\n",
      "-------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 190  |\n",
      "|    iterations      | 1800 |\n",
      "|    time_elapsed    | 47   |\n",
      "|    total_timesteps | 9000 |\n",
      "-----------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 190       |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 49        |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0        |\n",
      "|    explained_variance | -4.29e-06 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | -0        |\n",
      "|    value_loss         | 1.43      |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 2.80 +/- 0.75\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.8      |\n",
      "|    mean_reward        | -100     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0       |\n",
      "|    explained_variance | 4.41e-06 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | -0       |\n",
      "|    value_loss         | 0.555    |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 190   |\n",
      "|    iterations      | 2000  |\n",
      "|    time_elapsed    | 52    |\n",
      "|    total_timesteps | 10000 |\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "#train agent using stable baselines3 A2C\n",
    "from stable_baselines3 import A2C\n",
    "\n",
    "env = gym.make('BinPack3D-v1', \n",
    "                container_size = (8, 4, 4),\n",
    "                boxSeqGenerator='fixed', \n",
    "                enabled_rotations = [Rotate.NOOP],\n",
    "                n_foreseeable_box = 1,\n",
    "                box_set = [Box(2,1,1), Box(2,1,1), Box(2,1,1), Box(1,2,1), Box(1,2,1)]\n",
    "            )\n",
    "vec_env = DummyVecEnv([lambda: env])\n",
    "# Create eval callback that evaluates agent for 5 episodes every 500 training environment steps.\n",
    "eval_callback = EvalCallback(env, best_model_save_path=EVAL_LOG_DIR,\n",
    "                              log_path=EVAL_LOG_DIR, eval_freq=1000, \n",
    "                              deterministic=True, render=False)\n",
    "\n",
    "model = A2C(\"MultiInputPolicy\", vec_env, verbose=1)\n",
    "model.learn(total_timesteps=10_000, callback=eval_callback)\n",
    "\n",
    "#save the model\n",
    "model.save(DATA_DIR+\"/a2c_model\")\n",
    "\n",
    "#run A2C prediction and create gif\n",
    "A2C_GIF_DIR = DATA_DIR+\"/a2c.gif\"\n",
    "a2c_model = A2C.load(DATA_DIR+\"/a2c_model\", env=vec_env)\n",
    "create_gif(a2c_model, vec_env, A2C_GIF_DIR, fps=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
