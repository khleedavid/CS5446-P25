{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dependencies for the project\n",
    "import gymnasium as gym\n",
    "import gym_BinPack3D\n",
    "from gym_BinPack3D.envs import Box, Rotate\n",
    "import os\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CWD = os.path.join(os.getcwd(), 'Documents/GitHub/CS5446_AI_planning/CS5446-Project')\n",
    "DATA_DIR = os.path.join(CWD, 'data')\n",
    "GIF_DIR = os.path.join(CWD, 'gifs')\n",
    "TENSORBOARD_DIR = os.path.join(CWD, 'tensorboard')\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(GIF_DIR, exist_ok=True)\n",
    "os.makedirs(TENSORBOARD_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using CUT-2 logic box sequence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dlkh/miniconda3/envs/cs5446_ws/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:42: UserWarning: \u001b[33mWARN: A Box observation space has an unconventional shape (neither an image, nor a 1D vector). We recommend flattening the observation to have only a 1D vector or use a custom policy to properly process the data. Actual observation shape: (3, 3)\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/dlkh/miniconda3/envs/cs5446_ws/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:42: UserWarning: \u001b[33mWARN: A Box observation space has an unconventional shape (neither an image, nor a 1D vector). We recommend flattening the observation to have only a 1D vector or use a custom policy to properly process the data. Actual observation shape: (25, 4)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(MultiDiscrete([100   4]),\n",
       " Dict('coming_boxes': Box(0.0, 25.0, (3, 3), float32), 'height_map': Box(0.0, 4.0, (25, 4), float32), 'valid_placement_mask': MultiBinary((4, 25, 4))))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#register the environment\n",
    "gym.envs.register(\n",
    "    id='BinPack3D-v1',\n",
    "    entry_point='gym_BinPack3D.envs:PackingGame',\n",
    ")\n",
    "\n",
    "#define the environment.\n",
    "#container_size: size of the container in 3D\n",
    "#boxSeqGenerator: how the boxes are generated.\n",
    "#enabled_rotations: which rotations are allowed for the boxes\n",
    "#n_foreseeable_box: how many boxes are shown to the agent\n",
    "#box_set: the set of boxes that are used in the environment. \n",
    "\n",
    "env = gym.make('BinPack3D-v1', \n",
    "                container_size = (25, 4, 4),\n",
    "                boxSeqGenerator='CUT-2', \n",
    "                enabled_rotations = [Rotate.NOOP, Rotate.XY, Rotate.XZ, Rotate.YZ],\n",
    "                n_foreseeable_box = 3,\n",
    "                minSideLen = 1,\n",
    "                maxSideLen = 2,\n",
    "            )\n",
    "\n",
    "\n",
    "env.action_space, env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Baseline Agent\n",
    "Here we load an agent with perfect information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using CUT-2 logic box sequence\n",
      "(0, 0) (0, <Rotate.NOOP: 0>) 50.0\n",
      "(1, 3) (7, <Rotate.NOOP: 0>) 49.0\n",
      "(17, 3) (71, <Rotate.NOOP: 0>) 33.0\n",
      "(0, 3) (3, <Rotate.NOOP: 0>) 50.0\n",
      "(9, 3) (39, <Rotate.NOOP: 0>) 41.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dlkh/miniconda3/envs/cs5446_ws/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:164: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting numpy array dtype to be float32, actual type: int64\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/dlkh/miniconda3/envs/cs5446_ws/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:188: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "/Users/dlkh/miniconda3/envs/cs5446_ws/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:164: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting numpy array dtype to be float32, actual type: int32\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/dlkh/miniconda3/envs/cs5446_ws/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:364: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/dlkh/miniconda3/envs/cs5446_ws/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:164: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting numpy array dtype to be float32, actual type: int64\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/dlkh/miniconda3/envs/cs5446_ws/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:188: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "/Users/dlkh/miniconda3/envs/cs5446_ws/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:164: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting numpy array dtype to be float32, actual type: int32\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 0) (40, <Rotate.NOOP: 0>) 40.0\n",
      "(13, 0) (52, <Rotate.NOOP: 0>) 37.0\n",
      "(4, 2) (18, <Rotate.NOOP: 0>) 46.0\n",
      "(12, 1) (49, <Rotate.NOOP: 0>) 38.0\n",
      "(14, 3) (59, <Rotate.NOOP: 0>) 36.0\n",
      "(21, 3) (87, <Rotate.NOOP: 0>) 29.0\n",
      "(16, 0) (64, <Rotate.NOOP: 0>) 34.0\n",
      "(15, 1) (61, <Rotate.NOOP: 0>) 35.0\n",
      "(12, 0) (48, <Rotate.NOOP: 0>) 38.0\n",
      "(15, 3) (63, <Rotate.NOOP: 0>) 35.0\n",
      "(9, 1) (37, <Rotate.NOOP: 0>) 41.0\n",
      "(3, 0) (12, <Rotate.NOOP: 0>) 47.0\n",
      "(24, 3) (99, <Rotate.NOOP: 0>) 26.0\n",
      "(23, 1) (93, <Rotate.NOOP: 0>) 27.0\n",
      "(6, 0) (24, <Rotate.NOOP: 0>) 44.0\n",
      "(13, 0) (52, <Rotate.NOOP: 0>) 37.0\n",
      "(16, 1) (65, <Rotate.NOOP: 0>) 34.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dlkh/Documents/GitHub/CS5446_AI_planning/CS5446-Project/gym-BinPack3D/gym_BinPack3D/envs/BinPack3DEnv.py:211: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18, 0) (72, <Rotate.NOOP: 0>) 32.0\n",
      "(23, 0) (92, <Rotate.NOOP: 0>) 27.0\n",
      "(1, 0) (4, <Rotate.NOOP: 0>) 49.0\n",
      "(3, 3) (15, <Rotate.NOOP: 0>) 47.0\n",
      "(22, 1) (89, <Rotate.NOOP: 0>) 28.0\n",
      "(22, 2) (90, <Rotate.NOOP: 0>) 28.0\n",
      "(9, 0) (36, <Rotate.NOOP: 0>) 41.0\n",
      "(9, 0) (36, <Rotate.NOOP: 0>) 41.0\n",
      "(4, 0) (16, <Rotate.NOOP: 0>) 46.0\n",
      "(20, 3) (83, <Rotate.NOOP: 0>) 30.0\n",
      "(24, 2) (98, <Rotate.NOOP: 0>) 26.0\n",
      "(15, 3) (63, <Rotate.NOOP: 0>) 35.0\n",
      "(0, 2) (2, <Rotate.NOOP: 0>) 50.0\n",
      "(0, 3) (3, <Rotate.NOOP: 0>) 50.0\n",
      "(4, 3) (19, <Rotate.NOOP: 0>) 46.0\n",
      "(0, 0) (0, <Rotate.NOOP: 0>) 50.0\n",
      "(18, 0) (72, <Rotate.NOOP: 0>) 32.0\n",
      "(23, 2) (94, <Rotate.NOOP: 0>) 27.0\n",
      "(14, 3) (59, <Rotate.NOOP: 0>) 36.0\n",
      "(11, 3) (47, <Rotate.NOOP: 0>) 39.0\n",
      "(9, 3) (39, <Rotate.NOOP: 0>) 41.0\n",
      "(5, 3) (23, <Rotate.NOOP: 0>) 45.0\n",
      "(7, 1) (29, <Rotate.NOOP: 0>) 43.0\n",
      "(2, 3) (11, <Rotate.NOOP: 0>) 48.0\n",
      "(7, 0) (28, <Rotate.NOOP: 0>) 43.0\n",
      "(24, 0) (96, <Rotate.NOOP: 0>) 26.0\n",
      "(6, 2) (26, <Rotate.NOOP: 0>) 44.0\n",
      "(20, 0) (80, <Rotate.NOOP: 0>) 30.0\n",
      "(21, 3) (87, <Rotate.NOOP: 0>) 29.0\n",
      "(11, 1) (45, <Rotate.NOOP: 0>) 39.0\n",
      "(21, 1) (85, <Rotate.NOOP: 0>) 29.0\n",
      "(5, 3) (23, <Rotate.NOOP: 0>) 45.0\n",
      "(19, 1) (77, <Rotate.NOOP: 0>) 31.0\n",
      "(19, 2) (78, <Rotate.NOOP: 0>) 31.0\n",
      "(15, 0) (60, <Rotate.NOOP: 0>) 35.0\n",
      "(13, 1) (53, <Rotate.NOOP: 0>) 37.0\n",
      "(18, 1) (73, <Rotate.NOOP: 0>) 32.0\n",
      "(23, 3) (95, <Rotate.NOOP: 0>) 27.0\n",
      "(21, 0) (84, <Rotate.NOOP: 0>) 29.0\n",
      "(18, 3) (75, <Rotate.NOOP: 0>) 32.0\n",
      "(9, 1) (37, <Rotate.NOOP: 0>) 41.0\n",
      "(12, 3) (51, <Rotate.NOOP: 0>) 38.0\n",
      "(15, 0) (60, <Rotate.NOOP: 0>) 35.0\n",
      "(19, 2) (78, <Rotate.NOOP: 0>) 31.0\n",
      "(7, 3) (31, <Rotate.NOOP: 0>) 43.0\n",
      "(1, 0) (4, <Rotate.NOOP: 0>) 49.0\n",
      "(10, 3) (43, <Rotate.NOOP: 0>) 40.0\n",
      "(7, 3) (31, <Rotate.NOOP: 0>) 43.0\n",
      "(23, 0) (92, <Rotate.NOOP: 0>) 27.0\n",
      "(17, 3) (71, <Rotate.NOOP: 0>) 33.0\n",
      "(19, 0) (76, <Rotate.NOOP: 0>) 31.0\n",
      "(2, 2) (10, <Rotate.NOOP: 0>) 48.0\n",
      "(12, 3) (51, <Rotate.NOOP: 0>) 38.0\n",
      "(7, 1) (29, <Rotate.NOOP: 0>) 43.0\n",
      "(21, 2) (86, <Rotate.NOOP: 0>) 29.0\n",
      "(14, 1) (57, <Rotate.NOOP: 0>) 36.0\n",
      "(11, 1) (45, <Rotate.NOOP: 0>) 39.0\n",
      "(4, 1) (17, <Rotate.NOOP: 0>) 46.0\n",
      "(11, 0) (44, <Rotate.NOOP: 0>) 39.0\n",
      "(24, 2) (98, <Rotate.NOOP: 0>) 26.0\n",
      "(4, 0) (16, <Rotate.NOOP: 0>) 46.0\n",
      "(5, 2) (22, <Rotate.NOOP: 0>) 45.0\n",
      "(17, 1) (69, <Rotate.NOOP: 0>) 33.0\n",
      "(1, 1) (5, <Rotate.NOOP: 0>) 49.0\n",
      "(16, 0) (64, <Rotate.NOOP: 0>) 34.0\n",
      "(10, 3) (43, <Rotate.NOOP: 0>) 40.0\n",
      "(19, 0) (76, <Rotate.NOOP: 0>) 31.0\n",
      "(7, 0) (28, <Rotate.NOOP: 0>) 43.0\n",
      "(0, 0) (0, <Rotate.NOOP: 0>) 50.0\n",
      "(12, 1) (49, <Rotate.NOOP: 0>) 38.0\n",
      "(23, 3) (95, <Rotate.NOOP: 0>) 27.0\n",
      "(16, 1) (65, <Rotate.NOOP: 0>) 34.0\n",
      "(0, 1) (1, <Rotate.NOOP: 0>) 50.0\n",
      "(5, 0) (20, <Rotate.NOOP: 0>) 45.0\n",
      "(2, 0) (8, <Rotate.NOOP: 0>) 48.0\n",
      "(6, 1) (25, <Rotate.NOOP: 0>) 44.0\n",
      "(0, 1) (1, <Rotate.NOOP: 0>) 50.0\n",
      "(18, 3) (75, <Rotate.NOOP: 0>) 32.0\n",
      "(21, 0) (84, <Rotate.NOOP: 0>) 29.0\n",
      "(21, 2) (86, <Rotate.NOOP: 0>) 29.0\n",
      "(4, 1) (17, <Rotate.NOOP: 0>) 46.0\n",
      "(2, 1) (9, <Rotate.NOOP: 0>) 48.0\n",
      "(18, 1) (73, <Rotate.NOOP: 0>) 32.0\n",
      "(19, 3) (79, <Rotate.NOOP: 0>) 31.0\n",
      "(19, 0) (76, <Rotate.NOOP: 0>) 31.0\n",
      "(23, 0) (92, <Rotate.NOOP: 0>) 27.0\n",
      "(6, 0) (24, <Rotate.NOOP: 0>) 44.0\n",
      "(2, 0) (8, <Rotate.NOOP: 0>) 48.0\n",
      "(4, 0) (16, <Rotate.NOOP: 0>) 46.0\n",
      "(21, 0) (84, <Rotate.NOOP: 0>) 29.0\n",
      "(3, 1) (13, <Rotate.NOOP: 0>) 47.0\n",
      "(2, 1) (9, <Rotate.NOOP: 0>) 48.0\n",
      "(0, 0) (0, <Rotate.NOOP: 0>) -200\n"
     ]
    }
   ],
   "source": [
    "#load the environment with a baseline agent\n",
    "env = gym.make('BinPack3D-v1', \n",
    "             container_size = (25, 4, 4),\n",
    "                boxSeqGenerator='CUT-2', \n",
    "                enabled_rotations = [Rotate.NOOP],\n",
    "                n_foreseeable_box = 3,\n",
    "                minSideLen = 1,\n",
    "                maxSideLen = 2,\n",
    "            )\n",
    "\n",
    "#default container size is (9, 11, 13), with maxSideLen = 5, minSideLen = 2\n",
    "#container size for our game is (25, 4, 4), with maxSideLen = 2, minSideLen = 1\n",
    "\n",
    "frames = []\n",
    "obs = env.reset()\n",
    "#set environment to render rgb_array\n",
    "frame = env.render()\n",
    "rewards = []\n",
    "while True:\n",
    "    frames.append(frame)\n",
    "    # we cheat the game by look at cut process info and get the \n",
    "    # correct pos to place box, achieving perfect packing\n",
    "    box = env.boxSeqGenerator.next_N_boxes()[0]\n",
    "    pos = (box.x,box.y)\n",
    "    act_pos = env.position_to_actionIdx(pos)\n",
    "    rot = Rotate.NOOP\n",
    "    action = (act_pos, rot)\n",
    "\n",
    "    obs, reward, done,_, info = env.step(action)\n",
    "    frame = env.render()\n",
    "    # print(reward,done,info)\n",
    "    print(pos, action, reward)\n",
    "    rewards.append(reward + rewards[-1] if len(rewards) > 0 else reward)\n",
    "    \n",
    "    if done: break\n",
    "        \n",
    "env.render()\n",
    "\n",
    "imageio.mimsave(GIF_DIR+\"/baseline.gif\", frames)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using wrapped reward function\n",
    "We define a perfect agent who has knowledge of the exact order of boxes to be placed in the environment by providing the 'CUT-2' sequence order.\n",
    "\n",
    "Then, we want to train an agent who does not have knowledge of the exact order of boxes to stack the incoming boxes.\n",
    "\n",
    "Modifying the reward, we train a PPO and A2C agent using stable_baselines3 to see if the agent is able to change its behavior based on its reward function.\n",
    "We plot the reward in comparison with the perfect agent to test its performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test using a wrapper to modify the reward\n",
    "\n",
    "from gymnasium import Wrapper\n",
    "\n",
    "class RewardWrapper(Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.container_volume = env.container_size[0] * env.container_size[1] * env.container_size[2]\n",
    "        self.floor_space = env.container_size[0] * env.container_size[1]\n",
    "        self.prev_floor_space = 0\n",
    "        print(f\"Container dimensions: {env.container_size}, Container volume: {self.container_volume}, Floor space: {self.floor_space}\")\n",
    "        \n",
    "    def step(self, action):\n",
    "        obs, reward, done, truncated, info = self.env.step(action)\n",
    "\n",
    "        base_reward = reward        \n",
    "        # Encourage starting from the back of the container and filling towards the front\n",
    "        x_penalty = (box.x / self.container.dx) * 5  # Increase penalty as x increases\n",
    "        #base reward is the current ratio of boxes placed in the container to the total volume of the container.\n",
    "        # we want to minimize the floor space used by the boxes and maximize the height of the boxes.\n",
    "        # we weight the height as a positive reward and the floor space as a negative reward.\n",
    "        # height should be modified by the max height of the current box.\n",
    "        ratio_height = obs['height_map'].max() / env.container_size[2]\n",
    "        # larger height is better. use sigmoid to reward the agent for placing boxes higher. limit the range of the reward to -1 to 1.\n",
    "        if ratio_height > 1: #this means that the agent has placed a box that is higher than the container. we should penalize this heavily.\n",
    "            curr_max_height = -100\n",
    "        else:\n",
    "            curr_max_height = np.exp(ratio_height**2) + 1\n",
    "        \n",
    "        # floor space should be modified by the current floor space of the container. we want to minimize this and penalize the agent for using more floor space.\n",
    "        curr_floor_space = obs['valid_placement_mask'].sum() / self.floor_space\n",
    "        # smaller floor space is better. use sigmoid to penalize the agent for using more floor space.\n",
    "        curr_floor_space = -np.exp(-curr_floor_space**2) * 5\n",
    "\n",
    "        #reward longer sequences of boxes placed in the container. we want to maximize the number of boxes placed in the container.\n",
    "        curr_count = info['counter']\n",
    "        # larger count is better. use sigmoid to reward the agent for placing more boxes. limit the range of the reward to -1 to 1.\n",
    "        reward_count = (1+np.exp(-curr_count))**-1\n",
    "\n",
    "\n",
    "        #if action placement is near the x axis, reward the agent\n",
    "        #get the position of the action in x,y coordinates, and express the x coordinate as a fraction of the container length.\n",
    "        act_pos = self.env.actionIdx_to_position(action[0])[0] / self.env.container_size[0] -1\n",
    "        #smaller x position is better. use sigmoid to reward the agent for placing boxes near the x axis. limit the range of the reward to -1 to 1.\n",
    "        act_x_dist = np.exp(-act_pos**2) *5\n",
    "\n",
    "        #modify the reward. base reward is the current ratio of boxes placed in the container to the total volume of the container.\n",
    "        reward = base_reward + curr_max_height + curr_floor_space + reward_count + act_x_dist - x_penalty\n",
    "        if done:\n",
    "            reward = -100\n",
    "\n",
    "        #add reward details to the info dictionary\n",
    "        info['base_reward'] = base_reward\n",
    "        info['max_height'] = curr_max_height\n",
    "        info['floor_space'] = curr_floor_space\n",
    "        info['action_x_dist'] = act_x_dist\n",
    "        info['reward_count'] = reward_count\n",
    "        info['x_penalty'] = x_penalty\n",
    "\n",
    "        # print(f\"Base reward: {base_reward}, Modified reward: {reward}, Max height: {curr_max_height}, Floor space: {curr_floor_space}\")\n",
    "        \n",
    "        return obs, reward, done, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using CUT-2 logic box sequence\n",
      "Container dimensions: (25, 4, 4), Container volume: 400, Floor space: 100\n"
     ]
    }
   ],
   "source": [
    "#use wrapped_env to train a PPO agent\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3 import A2C\n",
    "#import tensorboard for logging\n",
    "\n",
    "base_env = gym.make('BinPack3D-v1',\n",
    "                container_size = (25, 4, 4), #(9, 11, 13),\n",
    "                    boxSeqGenerator='CUT-2', \n",
    "                    enabled_rotations = [Rotate.NOOP],\n",
    "                    n_foreseeable_box = 3,\n",
    "                    minSideLen = 1, # 2\n",
    "                    maxSideLen = 2, # 5\n",
    "                )\n",
    "wrapped_env = RewardWrapper(base_env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to /Users/dlkh/Documents/GitHub/CS5446_AI_planning/CS5446-Project/tensorboard/ppo_binpack3d_tensorboard/PPO_6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 4.75     |\n",
      "|    ep_rew_mean     | 58.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 431      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.47        |\n",
      "|    ep_rew_mean          | 90.4        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 414         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013257179 |\n",
      "|    clip_fraction        | 0.148       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.6        |\n",
      "|    explained_variance   | 0.000536    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.44e+03    |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0417     |\n",
      "|    value_loss           | 1.65e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.8         |\n",
      "|    ep_rew_mean          | 103         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 414         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 14          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014473887 |\n",
      "|    clip_fraction        | 0.153       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.57       |\n",
      "|    explained_variance   | 0.0271      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.84e+03    |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0428     |\n",
      "|    value_loss           | 1.57e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.77       |\n",
      "|    ep_rew_mean          | 146        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 421        |\n",
      "|    iterations           | 4          |\n",
      "|    time_elapsed         | 19         |\n",
      "|    total_timesteps      | 8192       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01357197 |\n",
      "|    clip_fraction        | 0.126      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -4.54      |\n",
      "|    explained_variance   | 0.0595     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 7.86e+03   |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | -0.0354    |\n",
      "|    value_loss           | 1.66e+04   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.13        |\n",
      "|    ep_rew_mean          | 122         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 425         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 24          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012553588 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.5        |\n",
      "|    explained_variance   | 0.0685      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.73e+03    |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0307     |\n",
      "|    value_loss           | 1.76e+04    |\n",
      "-----------------------------------------\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to /Users/dlkh/Documents/GitHub/CS5446_AI_planning/CS5446-Project/tensorboard/a2c_binpack3d_tensorboard/A2C_3\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.1      |\n",
      "|    ep_rew_mean        | 122      |\n",
      "| time/                 |          |\n",
      "|    fps                | 421      |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 500      |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.27    |\n",
      "|    explained_variance | 0.0315   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | 8.29     |\n",
      "|    value_loss         | 6.18e+03 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.66     |\n",
      "|    ep_rew_mean        | 146      |\n",
      "| time/                 |          |\n",
      "|    fps                | 424      |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.21    |\n",
      "|    explained_variance | 0.0844   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | 342      |\n",
      "|    value_loss         | 1.26e+04 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.73     |\n",
      "|    ep_rew_mean        | 151      |\n",
      "| time/                 |          |\n",
      "|    fps                | 424      |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.09    |\n",
      "|    explained_variance | -0.00143 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | 652      |\n",
      "|    value_loss         | 2.2e+04  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.43     |\n",
      "|    ep_rew_mean        | 183      |\n",
      "| time/                 |          |\n",
      "|    fps                | 439      |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -3.98    |\n",
      "|    explained_variance | 0.0534   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | -84.8    |\n",
      "|    value_loss         | 4.74e+03 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.23     |\n",
      "|    ep_rew_mean        | 175      |\n",
      "| time/                 |          |\n",
      "|    fps                | 440      |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -3.92    |\n",
      "|    explained_variance | 0.103    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | -72.2    |\n",
      "|    value_loss         | 4.41e+03 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.53     |\n",
      "|    ep_rew_mean        | 143      |\n",
      "| time/                 |          |\n",
      "|    fps                | 437      |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -3.78    |\n",
      "|    explained_variance | 0.325    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | 93.1     |\n",
      "|    value_loss         | 5.96e+03 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.73     |\n",
      "|    ep_rew_mean        | 156      |\n",
      "| time/                 |          |\n",
      "|    fps                | 438      |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -3.68    |\n",
      "|    explained_variance | 0.308    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | 143      |\n",
      "|    value_loss         | 7.13e+03 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.73     |\n",
      "|    ep_rew_mean        | 157      |\n",
      "| time/                 |          |\n",
      "|    fps                | 437      |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 9        |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -3.54    |\n",
      "|    explained_variance | 0.299    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | -54.8    |\n",
      "|    value_loss         | 3.2e+03  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.37     |\n",
      "|    ep_rew_mean        | 182      |\n",
      "| time/                 |          |\n",
      "|    fps                | 440      |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 10       |\n",
      "|    total_timesteps    | 4500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -3.54    |\n",
      "|    explained_variance | 0.28     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | -175     |\n",
      "|    value_loss         | 5.97e+03 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.81     |\n",
      "|    ep_rew_mean        | 201      |\n",
      "| time/                 |          |\n",
      "|    fps                | 441      |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 11       |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -3.42    |\n",
      "|    explained_variance | 0.0606   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | 390      |\n",
      "|    value_loss         | 1.85e+04 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.65     |\n",
      "|    ep_rew_mean        | 190      |\n",
      "| time/                 |          |\n",
      "|    fps                | 443      |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 12       |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -3.75    |\n",
      "|    explained_variance | 0.00574  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | 182      |\n",
      "|    value_loss         | 1.42e+04 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.82     |\n",
      "|    ep_rew_mean        | 197      |\n",
      "| time/                 |          |\n",
      "|    fps                | 445      |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 13       |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -3.23    |\n",
      "|    explained_variance | 0.0131   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | -164     |\n",
      "|    value_loss         | 5.86e+03 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.27     |\n",
      "|    ep_rew_mean        | 174      |\n",
      "| time/                 |          |\n",
      "|    fps                | 447      |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 14       |\n",
      "|    total_timesteps    | 6500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -3.46    |\n",
      "|    explained_variance | 0.0285   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | -141     |\n",
      "|    value_loss         | 5.51e+03 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.25     |\n",
      "|    ep_rew_mean        | 207      |\n",
      "| time/                 |          |\n",
      "|    fps                | 451      |\n",
      "|    iterations         | 1400     |\n",
      "|    time_elapsed       | 15       |\n",
      "|    total_timesteps    | 7000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.14    |\n",
      "|    explained_variance | 0.0293   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | -191     |\n",
      "|    value_loss         | 6.39e+03 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.87     |\n",
      "|    ep_rew_mean        | 190      |\n",
      "| time/                 |          |\n",
      "|    fps                | 453      |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 16       |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -3.19    |\n",
      "|    explained_variance | -0.00351 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | -196     |\n",
      "|    value_loss         | 6.76e+03 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.22     |\n",
      "|    ep_rew_mean        | 211      |\n",
      "| time/                 |          |\n",
      "|    fps                | 456      |\n",
      "|    iterations         | 1600     |\n",
      "|    time_elapsed       | 17       |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -3.14    |\n",
      "|    explained_variance | 0.689    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | 291      |\n",
      "|    value_loss         | 9.5e+03  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.21     |\n",
      "|    ep_rew_mean        | 210      |\n",
      "| time/                 |          |\n",
      "|    fps                | 458      |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 18       |\n",
      "|    total_timesteps    | 8500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -3.39    |\n",
      "|    explained_variance | 0.608    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | 189      |\n",
      "|    value_loss         | 5.48e+03 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.5      |\n",
      "|    ep_rew_mean        | 178      |\n",
      "| time/                 |          |\n",
      "|    fps                | 459      |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 19       |\n",
      "|    total_timesteps    | 9000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -3.55    |\n",
      "|    explained_variance | 0.535    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | 61.6     |\n",
      "|    value_loss         | 2.55e+03 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.07     |\n",
      "|    ep_rew_mean        | 203      |\n",
      "| time/                 |          |\n",
      "|    fps                | 461      |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 20       |\n",
      "|    total_timesteps    | 9500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -3.04    |\n",
      "|    explained_variance | 0.146    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | 75.7     |\n",
      "|    value_loss         | 9.67e+03 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.81     |\n",
      "|    ep_rew_mean        | 192      |\n",
      "| time/                 |          |\n",
      "|    fps                | 462      |\n",
      "|    iterations         | 2000     |\n",
      "|    time_elapsed       | 21       |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.17    |\n",
      "|    explained_variance | 0.0559   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | 574      |\n",
      "|    value_loss         | 1.81e+04 |\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model_PPO = PPO(\"MultiInputPolicy\", wrapped_env, verbose=1, tensorboard_log=TENSORBOARD_DIR+\"/ppo_binpack3d_tensorboard/\")\n",
    "model_PPO.learn(total_timesteps=100000)\n",
    "\n",
    "#use wrapped_env to train a A2C agent\n",
    "model_a2c = A2C(\"MultiInputPolicy\", wrapped_env, verbose=1, tensorboard_log=TENSORBOARD_DIR+\"/a2c_binpack3d_tensorboard/\")\n",
    "model_a2c.learn(total_timesteps=100000)\n",
    "\n",
    "#save the model\n",
    "model_PPO.save(DATA_DIR+\"/ppo_reward_wrapper\")\n",
    "model_a2c.save(DATA_DIR+\"/a2c_reward_wrapper\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Step: 0, Best frame length: 0\n",
      "info: {'counter': 7, 'ratio': 0.065, 'base_reward': -200, 'max_height': 2.2840254166877414, 'floor_space': -4.360287137573596, 'action_x_dist': 4.1199371666585165, 'reward_count': 0.9990889488055994, 'x_penalty': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dlkh/Documents/GitHub/CS5446_AI_planning/CS5446-Project/gym-BinPack3D/gym_BinPack3D/envs/BinPack3DEnv.py:211: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "info: {'counter': 16, 'ratio': 0.135, 'base_reward': -200, 'max_height': 3.718281828459045, 'floor_space': -3.148851907005015, 'action_x_dist': 3.488381630355155, 'reward_count': 0.9999998874648379, 'x_penalty': 0.0}\n",
      "info: {'counter': 17, 'ratio': 0.1175, 'base_reward': -200, 'max_height': 3.718281828459045, 'floor_space': -3.2770312716342027, 'action_x_dist': 3.488381630355155, 'reward_count': 0.9999999586006244, 'x_penalty': 0.0}\n",
      "info: {'counter': 19, 'ratio': 0.0875, 'base_reward': -200, 'max_height': 2.2840254166877414, 'floor_space': -3.2770312716342027, 'action_x_dist': 4.992006396588032, 'reward_count': 0.9999999943972036, 'x_penalty': 0.0}\n",
      "Step: 100, Best frame length: 20\n",
      "Step: 200, Best frame length: 20\n",
      "info: {'counter': 21, 'ratio': 0.21, 'base_reward': -200, 'max_height': 3.718281828459045, 'floor_space': -3.488381630355155, 'action_x_dist': 3.1488519070050156, 'reward_count': 0.9999999992417439, 'x_penalty': 0.0}\n",
      "info: {'counter': 29, 'ratio': 0.285, 'base_reward': -200, 'max_height': 3.718281828459045, 'floor_space': -4.226346517639093, 'action_x_dist': 4.1199371666585165, 'reward_count': 0.9999999999997455, 'x_penalty': 0.0}\n",
      "Step: 300, Best frame length: 30\n",
      "Step: 400, Best frame length: 30\n",
      "Best ppo len:  30\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Step: 0, Best frame length: 0\n",
      "info: {'counter': 9, 'ratio': 0.06, 'base_reward': -200, 'max_height': 2.7550546569602985, 'floor_space': -2.848914123654615, 'action_x_dist': 4.51334206040471, 'reward_count': 0.9998766054240137, 'x_penalty': 0.0}\n",
      "info: {'counter': 13, 'ratio': 0.1675, 'base_reward': -200, 'max_height': 3.718281828459045, 'floor_space': -4.569655926356141, 'action_x_dist': 2.144781993395365, 'reward_count': 0.999997739675702, 'x_penalty': 0.0}\n",
      "info: {'counter': 17, 'ratio': 0.0825, 'base_reward': -200, 'max_height': 3.718281828459045, 'floor_space': -4.569655926356141, 'action_x_dist': 2.144781993395365, 'reward_count': 0.9999999586006244, 'x_penalty': 0.0}\n",
      "Step: 100, Best frame length: 18\n",
      "info: {'counter': 18, 'ratio': 0.215, 'base_reward': -200, 'max_height': 3.718281828459045, 'floor_space': -2.184392837666111, 'action_x_dist': 2.304901431054168, 'reward_count': 0.9999999847700205, 'x_penalty': 0.0}\n",
      "Step: 200, Best frame length: 19\n",
      "info: {'counter': 22, 'ratio': 0.1925, 'base_reward': -200, 'max_height': 3.718281828459045, 'floor_space': -2.105468299439559, 'action_x_dist': 4.720137414589178, 'reward_count': 0.9999999997210531, 'x_penalty': 0.0}\n",
      "Step: 300, Best frame length: 23\n",
      "Step: 400, Best frame length: 23\n",
      "Best a2c len:  23\n"
     ]
    }
   ],
   "source": [
    "#load the PPO model\n",
    "model_PPO = PPO.load(DATA_DIR+\"/ppo_reward_wrapper\", env=wrapped_env)\n",
    "best_frames = []\n",
    "best_PPO_rewards = []\n",
    "obs = model_PPO.env.reset()\n",
    "frame = model_PPO.env.render(mode=\"rgb_array\")\n",
    "done = False\n",
    "#save the longest sequence of frames for the gif\n",
    "max_framelen = 0\n",
    "for i in range(500):\n",
    "    #count every 100 steps\n",
    "    if i % 100 == 0:\n",
    "        print(f\"Step: {i}, Best frame length: {max_framelen}\")\n",
    "    PPO_rewards = []\n",
    "    frames = []\n",
    "    while not done:\n",
    "        frames.append(frame)\n",
    "        action, _state = model_PPO.predict(obs, deterministic=False)\n",
    "        obs, reward, done,_, info = wrapped_env.step(action.flatten())\n",
    "        # print(obs, reward, done, info)\n",
    "        PPO_rewards.append(reward + PPO_rewards[-1] if len(PPO_rewards) > 0 else reward)\n",
    "        frame = model_PPO.env.render(mode=\"rgb_array\")\n",
    "        if frame is None:\n",
    "            print(\"Frame is None!!\")\n",
    "            break\n",
    "        # VecEnv resets automatically\n",
    "        if done:\n",
    "            obs = model_PPO.env.reset()\n",
    "    #save the longest sequence of frames for the gif\n",
    "    if len(frames) > max_framelen:\n",
    "        max_framelen = len(frames)\n",
    "        best_frames = frames\n",
    "        best_PPO_rewards = PPO_rewards\n",
    "        print(f'info: {info}')\n",
    "    else:\n",
    "        frames = []\n",
    "        done = False\n",
    "\n",
    "print(\"Best ppo len: \", len(best_frames))\n",
    "imageio.mimsave(GIF_DIR+\"/ppo_wrapper\"+\".gif\", best_frames)\n",
    "\n",
    "#load the A2C model\n",
    "model_a2c = A2C.load(DATA_DIR+\"/a2c_reward_wrapper\", env=wrapped_env)\n",
    "best_frames = []\n",
    "best_A2C_rewards = []\n",
    "obs = model_a2c.env.reset()\n",
    "frame = model_a2c.env.render(mode=\"rgb_array\")\n",
    "done = False\n",
    "#save the longest sequence of frames for the gif\n",
    "max_framelen = 0\n",
    "for i in range(500):\n",
    "    #count every 100 steps\n",
    "    if i % 100 == 0:\n",
    "        print(f\"Step: {i}, Best frame length: {max_framelen}\")\n",
    "    A2C_rewards = []\n",
    "    frames = []\n",
    "    while not done:\n",
    "        frames.append(frame)\n",
    "        action, _state = model_a2c.predict(obs, deterministic=False)\n",
    "        obs, reward, done,_, info = wrapped_env.step(action.flatten())\n",
    "        # print(obs, reward, done, info)\n",
    "        A2C_rewards.append(reward + A2C_rewards[-1] if len(A2C_rewards) > 0 else reward)\n",
    "        frame = model_a2c.env.render(mode=\"rgb_array\")\n",
    "        if frame is None:\n",
    "            print(\"Frame is None!!\")\n",
    "            break\n",
    "        # VecEnv resets automatically\n",
    "        if done:\n",
    "            obs = model_a2c.env.reset()\n",
    "    #save the longest sequence of frames for the gif\n",
    "    if len(frames) > max_framelen:\n",
    "        max_framelen = len(frames)\n",
    "        best_frames = frames\n",
    "        best_A2C_rewards = A2C_rewards\n",
    "        print(f'info: {info}')\n",
    "    else:\n",
    "        frames = []\n",
    "        done = False\n",
    "\n",
    "print(\"Best a2c len: \", len(best_frames))\n",
    "imageio.mimsave(GIF_DIR+\"/a2c_wrapper\"+\".gif\", best_frames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using CUT-2 logic box sequence\n",
      "Container dimensions: (25, 4, 4), Container volume: 400, Floor space: 100\n",
      "{'counter': 1, 'ratio': 0.01, 'base_reward': 46.0, 'max_height': 2.2840254166877414, 'floor_space': -2.144781993395365, 'action_x_dist': 2.469060990166731, 'reward_count': 0.7310585786300049, 'x_penalty': 0.8}\n",
      "{'counter': 2, 'ratio': 0.02, 'base_reward': 29.0, 'max_height': 2.2840254166877414, 'floor_space': -3.488381630355155, 'action_x_dist': 4.8736245080089695, 'reward_count': 0.8807970779778823, 'x_penalty': 4.2}\n",
      "{'counter': 3, 'ratio': 0.04, 'base_reward': 27.0, 'max_height': 2.2840254166877414, 'floor_space': -2.2242903311147053, 'action_x_dist': 4.968102181895745, 'reward_count': 0.9525741268224334, 'x_penalty': 4.6000000000000005}\n",
      "{'counter': 4, 'ratio': 0.05, 'base_reward': 35.0, 'max_height': 2.2840254166877414, 'floor_space': -2.304901431054168, 'action_x_dist': 4.260718944831057, 'reward_count': 0.9820137900379085, 'x_penalty': 3.0}\n",
      "{'counter': 5, 'ratio': 0.06, 'base_reward': 30.0, 'max_height': 2.2840254166877414, 'floor_space': -3.3620063119850134, 'action_x_dist': 4.803947195761616, 'reward_count': 0.9933071490757153, 'x_penalty': 4.0}\n",
      "{'counter': 6, 'ratio': 0.07, 'base_reward': 41.0, 'max_height': 2.2840254166877414, 'floor_space': -3.446426552733132, 'action_x_dist': 3.3195788166773674, 'reward_count': 0.9975273768433653, 'x_penalty': 1.7999999999999998}\n",
      "{'counter': 7, 'ratio': 0.08, 'base_reward': 40.0, 'max_height': 2.2840254166877414, 'floor_space': -3.9710792630827334, 'action_x_dist': 3.488381630355155, 'reward_count': 0.9990889488055994, 'x_penalty': 2.0}\n",
      "{'counter': 8, 'ratio': 0.1, 'base_reward': 29.0, 'max_height': 2.2840254166877414, 'floor_space': -2.636462120215243, 'action_x_dist': 4.8736245080089695, 'reward_count': 0.9996646498695336, 'x_penalty': 4.2}\n",
      "{'counter': 9, 'ratio': 0.11, 'base_reward': 45.0, 'max_height': 2.2840254166877414, 'floor_space': -2.7211046316260368, 'action_x_dist': 2.636462120215243, 'reward_count': 0.9998766054240137, 'x_penalty': 1.0}\n",
      "{'counter': 10, 'ratio': 0.12, 'base_reward': 27.0, 'max_height': 2.2840254166877414, 'floor_space': -1.8393972058572117, 'action_x_dist': 4.968102181895745, 'reward_count': 0.9999546021312976, 'x_penalty': 4.6000000000000005}\n",
      "{'counter': 11, 'ratio': 0.1225, 'base_reward': 42.0, 'max_height': 2.2840254166877414, 'floor_space': -4.008984279834707, 'action_x_dist': 3.1488519070050156, 'reward_count': 0.999983298578152, 'x_penalty': 1.6}\n",
      "{'counter': 12, 'ratio': 0.1425, 'base_reward': 50.0, 'max_height': 2.2840254166877414, 'floor_space': -4.191413016121167, 'action_x_dist': 1.8393972058572117, 'reward_count': 0.9999938558253978, 'x_penalty': 0.0}\n",
      "{'counter': 13, 'ratio': 0.1625, 'base_reward': 41.0, 'max_height': 3.718281828459045, 'floor_space': -1.9894096022560235, 'action_x_dist': 3.3195788166773674, 'reward_count': 0.999997739675702, 'x_penalty': 1.7999999999999998}\n",
      "{'counter': 14, 'ratio': 0.165, 'base_reward': 43.0, 'max_height': 3.718281828459045, 'floor_space': -1.9894096022560235, 'action_x_dist': 2.977362711196349, 'reward_count': 0.9999991684719722, 'x_penalty': 1.4000000000000001}\n",
      "{'counter': 15, 'ratio': 0.17, 'base_reward': 26.0, 'max_height': 3.718281828459045, 'floor_space': -1.9894096022560235, 'action_x_dist': 4.992006396588032, 'reward_count': 0.999999694097773, 'x_penalty': 4.8}\n",
      "{'counter': 16, 'ratio': 0.175, 'base_reward': 36.0, 'max_height': 3.718281828459045, 'floor_space': -1.9894096022560235, 'action_x_dist': 4.1199371666585165, 'reward_count': 0.9999998874648379, 'x_penalty': 2.8000000000000003}\n",
      "{'counter': 17, 'ratio': 0.18, 'base_reward': 31.0, 'max_height': 3.718281828459045, 'floor_space': -3.8153710180066804, 'action_x_dist': 4.720137414589178, 'reward_count': 0.9999999586006244, 'x_penalty': 3.8}\n",
      "{'counter': 18, 'ratio': 0.19, 'base_reward': 42.0, 'max_height': 3.718281828459045, 'floor_space': -3.8548724225005766, 'action_x_dist': 3.1488519070050156, 'reward_count': 0.9999999847700205, 'x_penalty': 1.6}\n",
      "{'counter': 19, 'ratio': 0.2, 'base_reward': 36.0, 'max_height': 3.718281828459045, 'floor_space': -1.9894096022560235, 'action_x_dist': 4.1199371666585165, 'reward_count': 0.9999999943972036, 'x_penalty': 2.8000000000000003}\n",
      "{'counter': 20, 'ratio': 0.205, 'base_reward': 34.0, 'max_height': 3.718281828459045, 'floor_space': -4.54186587104634, 'action_x_dist': 4.392233696749656, 'reward_count': 0.9999999979388463, 'x_penalty': 3.2}\n",
      "{'counter': 21, 'ratio': 0.225, 'base_reward': 48.0, 'max_height': 3.718281828459045, 'floor_space': -3.234382123810649, 'action_x_dist': 2.144781993395365, 'reward_count': 0.9999999992417439, 'x_penalty': 0.4}\n",
      "{'counter': 22, 'ratio': 0.23, 'base_reward': 45.0, 'max_height': 3.718281828459045, 'floor_space': -4.04644174085666, 'action_x_dist': 2.636462120215243, 'reward_count': 0.9999999997210531, 'x_penalty': 1.0}\n",
      "{'counter': 23, 'ratio': 0.24, 'base_reward': 29.0, 'max_height': 3.718281828459045, 'floor_space': -4.596696587832591, 'action_x_dist': 4.8736245080089695, 'reward_count': 0.9999999998973812, 'x_penalty': 4.2}\n",
      "{'counter': 24, 'ratio': 0.26, 'base_reward': 45.0, 'max_height': 3.718281828459045, 'floor_space': -4.622972573801054, 'action_x_dist': 2.636462120215243, 'reward_count': 0.9999999999622486, 'x_penalty': 1.0}\n",
      "{'counter': 25, 'ratio': 0.28, 'base_reward': 28.0, 'max_height': 3.718281828459045, 'floor_space': -3.488381630355155, 'action_x_dist': 4.928515920612215, 'reward_count': 0.999999999986112, 'x_penalty': 4.4}\n",
      "{'counter': 26, 'ratio': 0.285, 'base_reward': 38.0, 'max_height': 3.718281828459045, 'floor_space': -4.763763049016055, 'action_x_dist': 3.8153710180066804, 'reward_count': 0.999999999994891, 'x_penalty': 2.4}\n",
      "{'counter': 27, 'ratio': 0.305, 'base_reward': 35.0, 'max_height': 3.718281828459045, 'floor_space': -3.571686568679788, 'action_x_dist': 4.260718944831057, 'reward_count': 0.9999999999981204, 'x_penalty': 3.0}\n",
      "{'counter': 28, 'ratio': 0.315, 'base_reward': 50.0, 'max_height': 3.718281828459045, 'floor_space': -4.260718944831057, 'action_x_dist': 1.8393972058572117, 'reward_count': 0.9999999999993086, 'x_penalty': 0.0}\n",
      "{'counter': 29, 'ratio': 0.325, 'base_reward': 30.0, 'max_height': 3.718281828459045, 'floor_space': -3.612998049950968, 'action_x_dist': 4.803947195761616, 'reward_count': 0.9999999999997455, 'x_penalty': 4.0}\n",
      "{'counter': 30, 'ratio': 0.335, 'base_reward': 29.0, 'max_height': 3.718281828459045, 'floor_space': -4.32770731110883, 'action_x_dist': 4.8736245080089695, 'reward_count': 0.9999999999999065, 'x_penalty': 4.2}\n",
      "{'counter': 31, 'ratio': 0.345, 'base_reward': 26.0, 'max_height': 3.718281828459045, 'floor_space': -2.386523996307229, 'action_x_dist': 4.992006396588032, 'reward_count': 0.9999999999999656, 'x_penalty': 4.8}\n",
      "{'counter': 32, 'ratio': 0.3475, 'base_reward': 39.0, 'max_height': 3.718281828459045, 'floor_space': -4.742374276128023, 'action_x_dist': 3.6540564711000196, 'reward_count': 0.9999999999999873, 'x_penalty': 2.2}\n",
      "{'counter': 33, 'ratio': 0.3675, 'base_reward': 36.0, 'max_height': 3.718281828459045, 'floor_space': -2.636462120215243, 'action_x_dist': 4.1199371666585165, 'reward_count': 0.9999999999999953, 'x_penalty': 2.8000000000000003}\n",
      "{'counter': 34, 'ratio': 0.3725, 'base_reward': 48.0, 'max_height': 3.718281828459045, 'floor_space': -2.5524097487114465, 'action_x_dist': 2.144781993395365, 'reward_count': 0.9999999999999982, 'x_penalty': 0.4}\n",
      "{'counter': 35, 'ratio': 0.375, 'base_reward': 50.0, 'max_height': 3.718281828459045, 'floor_space': -4.822719171384077, 'action_x_dist': 1.8393972058572117, 'reward_count': 0.9999999999999993, 'x_penalty': 0.0}\n",
      "{'counter': 36, 'ratio': 0.395, 'base_reward': 32.0, 'max_height': 3.718281828459045, 'floor_space': -4.1199371666585165, 'action_x_dist': 4.622972573801054, 'reward_count': 0.9999999999999998, 'x_penalty': 3.5999999999999996}\n",
      "{'counter': 37, 'ratio': 0.405, 'base_reward': 33.0, 'max_height': 3.718281828459045, 'floor_space': -4.008984279834707, 'action_x_dist': 4.51334206040471, 'reward_count': 1.0, 'x_penalty': 3.4000000000000004}\n",
      "{'counter': 38, 'ratio': 0.41, 'base_reward': 36.0, 'max_height': 3.718281828459045, 'floor_space': -4.04644174085666, 'action_x_dist': 4.1199371666585165, 'reward_count': 1.0, 'x_penalty': 2.8000000000000003}\n",
      "{'counter': 39, 'ratio': 0.415, 'base_reward': 38.0, 'max_height': 3.718281828459045, 'floor_space': -4.008984279834707, 'action_x_dist': 3.8153710180066804, 'reward_count': 1.0, 'x_penalty': 2.4}\n",
      "{'counter': 40, 'ratio': 0.425, 'base_reward': 37.0, 'max_height': 3.718281828459045, 'floor_space': -4.648468973784771, 'action_x_dist': 3.9710792630827334, 'reward_count': 1.0, 'x_penalty': 2.6}\n",
      "{'counter': 41, 'ratio': 0.435, 'base_reward': 43.0, 'max_height': 3.718281828459045, 'floor_space': -2.7636103215169694, 'action_x_dist': 2.977362711196349, 'reward_count': 1.0, 'x_penalty': 1.4000000000000001}\n",
      "{'counter': 42, 'ratio': 0.44, 'base_reward': 30.0, 'max_height': 3.718281828459045, 'floor_space': -4.784291334309549, 'action_x_dist': 4.803947195761616, 'reward_count': 1.0, 'x_penalty': 4.0}\n",
      "{'counter': 43, 'ratio': 0.46, 'base_reward': 44.0, 'max_height': 3.718281828459045, 'floor_space': -4.6731712625872435, 'action_x_dist': 2.8062186822161745, 'reward_count': 1.0, 'x_penalty': 1.2}\n",
      "{'counter': 44, 'ratio': 0.47, 'base_reward': 46.0, 'max_height': 3.718281828459045, 'floor_space': -3.0202448746901265, 'action_x_dist': 2.469060990166731, 'reward_count': 1.0, 'x_penalty': 0.8}\n",
      "{'counter': 45, 'ratio': 0.475, 'base_reward': 36.0, 'max_height': 3.718281828459045, 'floor_space': -3.063131970922081, 'action_x_dist': 4.1199371666585165, 'reward_count': 1.0, 'x_penalty': 2.8000000000000003}\n",
      "{'counter': 46, 'ratio': 0.48, 'base_reward': 33.0, 'max_height': 3.718281828459045, 'floor_space': -4.720137414589178, 'action_x_dist': 4.51334206040471, 'reward_count': 1.0, 'x_penalty': 3.4000000000000004}\n",
      "{'counter': 47, 'ratio': 0.49, 'base_reward': 33.0, 'max_height': 3.718281828459045, 'floor_space': -4.803947195761616, 'action_x_dist': 4.51334206040471, 'reward_count': 1.0, 'x_penalty': 3.4000000000000004}\n",
      "{'counter': 48, 'ratio': 0.51, 'base_reward': 48.0, 'max_height': 3.718281828459045, 'floor_space': -4.260718944831057, 'action_x_dist': 2.144781993395365, 'reward_count': 1.0, 'x_penalty': 0.4}\n",
      "{'counter': 49, 'ratio': 0.515, 'base_reward': 47.0, 'max_height': 3.718281828459045, 'floor_space': -4.742374276128023, 'action_x_dist': 2.304901431054168, 'reward_count': 1.0, 'x_penalty': 0.6}\n",
      "{'counter': 50, 'ratio': 0.525, 'base_reward': 42.0, 'max_height': 3.718281828459045, 'floor_space': -4.360287137573596, 'action_x_dist': 3.1488519070050156, 'reward_count': 1.0, 'x_penalty': 1.6}\n",
      "{'counter': 51, 'ratio': 0.535, 'base_reward': 35.0, 'max_height': 3.718281828459045, 'floor_space': -4.423529524717418, 'action_x_dist': 4.260718944831057, 'reward_count': 1.0, 'x_penalty': 3.0}\n",
      "{'counter': 52, 'ratio': 0.545, 'base_reward': 50.0, 'max_height': 3.718281828459045, 'floor_space': -3.4042951454596238, 'action_x_dist': 1.8393972058572117, 'reward_count': 1.0, 'x_penalty': 0.0}\n",
      "{'counter': 53, 'ratio': 0.55, 'base_reward': 48.0, 'max_height': 3.718281828459045, 'floor_space': -4.392233696749656, 'action_x_dist': 2.144781993395365, 'reward_count': 1.0, 'x_penalty': 0.4}\n",
      "{'counter': 54, 'ratio': 0.56, 'base_reward': 47.0, 'max_height': 3.718281828459045, 'floor_space': -4.888756185966682, 'action_x_dist': 2.304901431054168, 'reward_count': 1.0, 'x_penalty': 0.6}\n",
      "{'counter': 55, 'ratio': 0.58, 'base_reward': 32.0, 'max_height': 3.718281828459045, 'floor_space': -4.54186587104634, 'action_x_dist': 4.622972573801054, 'reward_count': 1.0, 'x_penalty': 3.5999999999999996}\n",
      "{'counter': 56, 'ratio': 0.59, 'base_reward': 28.0, 'max_height': 3.718281828459045, 'floor_space': -4.484100475118934, 'action_x_dist': 4.928515920612215, 'reward_count': 1.0, 'x_penalty': 4.4}\n",
      "{'counter': 57, 'ratio': 0.6, 'base_reward': 30.0, 'max_height': 3.718281828459045, 'floor_space': -3.446426552733132, 'action_x_dist': 4.803947195761616, 'reward_count': 1.0, 'x_penalty': 4.0}\n",
      "{'counter': 58, 'ratio': 0.6025, 'base_reward': 41.0, 'max_height': 3.718281828459045, 'floor_space': -4.54186587104634, 'action_x_dist': 3.3195788166773674, 'reward_count': 1.0, 'x_penalty': 1.7999999999999998}\n",
      "{'counter': 59, 'ratio': 0.6075, 'base_reward': 43.0, 'max_height': 3.718281828459045, 'floor_space': -4.569655926356141, 'action_x_dist': 2.977362711196349, 'reward_count': 1.0, 'x_penalty': 1.4000000000000001}\n",
      "{'counter': 60, 'ratio': 0.6175, 'base_reward': 32.0, 'max_height': 3.718281828459045, 'floor_space': -4.484100475118934, 'action_x_dist': 4.622972573801054, 'reward_count': 1.0, 'x_penalty': 3.5999999999999996}\n",
      "{'counter': 61, 'ratio': 0.6225, 'base_reward': 40.0, 'max_height': 3.718281828459045, 'floor_space': -4.840596284582814, 'action_x_dist': 3.488381630355155, 'reward_count': 1.0, 'x_penalty': 2.0}\n",
      "{'counter': 62, 'ratio': 0.6325, 'base_reward': 39.0, 'max_height': 3.718281828459045, 'floor_space': -4.54186587104634, 'action_x_dist': 3.6540564711000196, 'reward_count': 1.0, 'x_penalty': 2.2}\n",
      "{'counter': 63, 'ratio': 0.6425, 'base_reward': 33.0, 'max_height': 3.718281828459045, 'floor_space': -4.54186587104634, 'action_x_dist': 4.51334206040471, 'reward_count': 1.0, 'x_penalty': 3.4000000000000004}\n",
      "{'counter': 64, 'ratio': 0.6475, 'base_reward': 41.0, 'max_height': 3.718281828459045, 'floor_space': -4.857568054851479, 'action_x_dist': 3.3195788166773674, 'reward_count': 1.0, 'x_penalty': 1.7999999999999998}\n",
      "{'counter': 65, 'ratio': 0.6575, 'base_reward': 39.0, 'max_height': 3.718281828459045, 'floor_space': -4.54186587104634, 'action_x_dist': 3.6540564711000196, 'reward_count': 1.0, 'x_penalty': 2.2}\n",
      "{'counter': 66, 'ratio': 0.6675, 'base_reward': 27.0, 'max_height': 3.718281828459045, 'floor_space': -4.569655926356141, 'action_x_dist': 4.968102181895745, 'reward_count': 1.0, 'x_penalty': 4.6000000000000005}\n",
      "{'counter': 67, 'ratio': 0.6775, 'base_reward': 43.0, 'max_height': 3.718281828459045, 'floor_space': -4.54186587104634, 'action_x_dist': 2.977362711196349, 'reward_count': 1.0, 'x_penalty': 1.4000000000000001}\n",
      "{'counter': 68, 'ratio': 0.6825, 'base_reward': 45.0, 'max_height': 3.718281828459045, 'floor_space': -4.9875156119873, 'action_x_dist': 2.636462120215243, 'reward_count': 1.0, 'x_penalty': 1.0}\n",
      "{'counter': 69, 'ratio': 0.7025, 'base_reward': 50.0, 'max_height': 3.718281828459045, 'floor_space': -4.9955020243926365, 'action_x_dist': 1.8393972058572117, 'reward_count': 1.0, 'x_penalty': 0.0}\n",
      "{'counter': 70, 'ratio': 0.7225, 'base_reward': 38.0, 'max_height': 3.718281828459045, 'floor_space': -4.1199371666585165, 'action_x_dist': 3.8153710180066804, 'reward_count': 1.0, 'x_penalty': 2.4}\n",
      "{'counter': 71, 'ratio': 0.7275, 'base_reward': 50.0, 'max_height': 3.718281828459045, 'floor_space': -4.596696587832591, 'action_x_dist': 1.8393972058572117, 'reward_count': 1.0, 'x_penalty': 0.0}\n",
      "{'counter': 72, 'ratio': 0.7325, 'base_reward': 37.0, 'max_height': 3.718281828459045, 'floor_space': -4.622972573801054, 'action_x_dist': 3.9710792630827334, 'reward_count': 1.0, 'x_penalty': 2.6}\n",
      "{'counter': 73, 'ratio': 0.7375, 'base_reward': 47.0, 'max_height': 3.718281828459045, 'floor_space': -4.6731712625872435, 'action_x_dist': 2.304901431054168, 'reward_count': 1.0, 'x_penalty': 0.6}\n",
      "{'counter': 74, 'ratio': 0.7475, 'base_reward': 40.0, 'max_height': 3.718281828459045, 'floor_space': -4.6731712625872435, 'action_x_dist': 3.488381630355155, 'reward_count': 1.0, 'x_penalty': 2.0}\n",
      "{'counter': 75, 'ratio': 0.7525, 'base_reward': 45.0, 'max_height': 3.718281828459045, 'floor_space': -4.742374276128023, 'action_x_dist': 2.636462120215243, 'reward_count': 1.0, 'x_penalty': 1.0}\n",
      "{'counter': 76, 'ratio': 0.7625, 'base_reward': 33.0, 'max_height': 3.718281828459045, 'floor_space': -4.294511931039239, 'action_x_dist': 4.51334206040471, 'reward_count': 1.0, 'x_penalty': 3.4000000000000004}\n",
      "{'counter': 77, 'ratio': 0.7675, 'base_reward': 31.0, 'max_height': 3.718281828459045, 'floor_space': -4.822719171384077, 'action_x_dist': 4.720137414589178, 'reward_count': 1.0, 'x_penalty': 3.8}\n",
      "{'counter': 78, 'ratio': 0.7775, 'base_reward': 47.0, 'max_height': 3.718281828459045, 'floor_space': -4.968102181895745, 'action_x_dist': 2.304901431054168, 'reward_count': 1.0, 'x_penalty': 0.6}\n",
      "{'counter': 79, 'ratio': 0.7875, 'base_reward': 45.0, 'max_height': 3.718281828459045, 'floor_space': -4.857568054851479, 'action_x_dist': 2.636462120215243, 'reward_count': 1.0, 'x_penalty': 1.0}\n",
      "{'counter': 80, 'ratio': 0.7975, 'base_reward': 39.0, 'max_height': 3.718281828459045, 'floor_space': -4.888756185966682, 'action_x_dist': 3.6540564711000196, 'reward_count': 1.0, 'x_penalty': 2.2}\n",
      "{'counter': 81, 'ratio': 0.8075, 'base_reward': 43.0, 'max_height': 3.718281828459045, 'floor_space': -4.569655926356141, 'action_x_dist': 2.977362711196349, 'reward_count': 1.0, 'x_penalty': 1.4000000000000001}\n",
      "{'counter': 82, 'ratio': 0.8125, 'base_reward': 26.0, 'max_height': 3.718281828459045, 'floor_space': -4.8736245080089695, 'action_x_dist': 4.992006396588032, 'reward_count': 1.0, 'x_penalty': 4.8}\n",
      "{'counter': 83, 'ratio': 0.8175, 'base_reward': 40.0, 'max_height': 3.718281828459045, 'floor_space': -4.928515920612215, 'action_x_dist': 3.488381630355155, 'reward_count': 1.0, 'x_penalty': 2.0}\n",
      "{'counter': 84, 'ratio': 0.8275, 'base_reward': 38.0, 'max_height': 3.718281828459045, 'floor_space': -4.648468973784771, 'action_x_dist': 3.8153710180066804, 'reward_count': 1.0, 'x_penalty': 2.4}\n",
      "{'counter': 85, 'ratio': 0.8325, 'base_reward': 35.0, 'max_height': 3.718281828459045, 'floor_space': -4.6731712625872435, 'action_x_dist': 4.260718944831057, 'reward_count': 1.0, 'x_penalty': 3.0}\n",
      "{'counter': 86, 'ratio': 0.8375, 'base_reward': 30.0, 'max_height': 3.718281828459045, 'floor_space': -4.939864553154191, 'action_x_dist': 4.803947195761616, 'reward_count': 1.0, 'x_penalty': 4.0}\n",
      "{'counter': 87, 'ratio': 0.8475, 'base_reward': 49.0, 'max_height': 3.718281828459045, 'floor_space': -4.484100475118934, 'action_x_dist': 1.9894096022560235, 'reward_count': 1.0, 'x_penalty': 0.2}\n",
      "{'counter': 88, 'ratio': 0.85, 'base_reward': 34.0, 'max_height': 3.718281828459045, 'floor_space': -4.9875156119873, 'action_x_dist': 4.392233696749656, 'reward_count': 1.0, 'x_penalty': 3.2}\n",
      "{'counter': 89, 'ratio': 0.86, 'base_reward': 30.0, 'max_height': 3.718281828459045, 'floor_space': -4.992006396588032, 'action_x_dist': 4.803947195761616, 'reward_count': 1.0, 'x_penalty': 4.0}\n",
      "{'counter': 90, 'ratio': 0.87, 'base_reward': 37.0, 'max_height': 3.718281828459045, 'floor_space': -4.999500024999167, 'action_x_dist': 3.9710792630827334, 'reward_count': 1.0, 'x_penalty': 2.6}\n",
      "{'counter': 91, 'ratio': 0.89, 'base_reward': 34.0, 'max_height': 3.718281828459045, 'floor_space': -4.697065314067379, 'action_x_dist': 4.392233696749656, 'reward_count': 1.0, 'x_penalty': 3.2}\n",
      "{'counter': 92, 'ratio': 0.8925, 'base_reward': 37.0, 'max_height': 3.718281828459045, 'floor_space': -4.902954156012142, 'action_x_dist': 3.9710792630827334, 'reward_count': 1.0, 'x_penalty': 2.6}\n",
      "{'counter': 93, 'ratio': 0.8975, 'base_reward': 44.0, 'max_height': 3.718281828459045, 'floor_space': -4.916210019596277, 'action_x_dist': 2.8062186822161745, 'reward_count': 1.0, 'x_penalty': 1.2}\n",
      "{'counter': 94, 'ratio': 0.9025, 'base_reward': 41.0, 'max_height': 3.718281828459045, 'floor_space': -4.928515920612215, 'action_x_dist': 3.3195788166773674, 'reward_count': 1.0, 'x_penalty': 1.7999999999999998}\n",
      "{'counter': 95, 'ratio': 0.9075, 'base_reward': 41.0, 'max_height': 3.718281828459045, 'floor_space': -4.939864553154191, 'action_x_dist': 3.3195788166773674, 'reward_count': 1.0, 'x_penalty': 1.7999999999999998}\n",
      "{'counter': 96, 'ratio': 0.9125, 'base_reward': 28.0, 'max_height': 3.718281828459045, 'floor_space': -4.720137414589178, 'action_x_dist': 4.928515920612215, 'reward_count': 1.0, 'x_penalty': 4.4}\n",
      "{'counter': 97, 'ratio': 0.915, 'base_reward': 45.0, 'max_height': 3.718281828459045, 'floor_space': -4.939864553154191, 'action_x_dist': 2.636462120215243, 'reward_count': 1.0, 'x_penalty': 1.0}\n",
      "{'counter': 98, 'ratio': 0.92, 'base_reward': 43.0, 'max_height': 3.718281828459045, 'floor_space': -4.763763049016055, 'action_x_dist': 2.977362711196349, 'reward_count': 1.0, 'x_penalty': 1.4000000000000001}\n",
      "{'counter': 99, 'ratio': 0.9225, 'base_reward': 46.0, 'max_height': 3.718281828459045, 'floor_space': -4.975559927079149, 'action_x_dist': 2.469060990166731, 'reward_count': 1.0, 'x_penalty': 0.8}\n",
      "{'counter': 100, 'ratio': 0.9275, 'base_reward': 49.0, 'max_height': 3.718281828459045, 'floor_space': -4.763763049016055, 'action_x_dist': 1.9894096022560235, 'reward_count': 1.0, 'x_penalty': 0.2}\n",
      "{'counter': 101, 'ratio': 0.93, 'base_reward': 39.0, 'max_height': 3.718281828459045, 'floor_space': -4.784291334309549, 'action_x_dist': 3.6540564711000196, 'reward_count': 1.0, 'x_penalty': 2.2}\n",
      "{'counter': 102, 'ratio': 0.9325, 'base_reward': 38.0, 'max_height': 3.718281828459045, 'floor_space': -4.803947195761616, 'action_x_dist': 3.8153710180066804, 'reward_count': 1.0, 'x_penalty': 2.4}\n",
      "{'counter': 103, 'ratio': 0.935, 'base_reward': 47.0, 'max_height': 3.718281828459045, 'floor_space': -4.999500024999167, 'action_x_dist': 2.304901431054168, 'reward_count': 1.0, 'x_penalty': 0.6}\n",
      "{'counter': 104, 'ratio': 0.945, 'base_reward': 38.0, 'max_height': 3.718281828459045, 'floor_space': -4.9955020243926365, 'action_x_dist': 3.8153710180066804, 'reward_count': 1.0, 'x_penalty': 2.4}\n",
      "{'counter': 105, 'ratio': 0.95, 'base_reward': 31.0, 'max_height': 3.718281828459045, 'floor_space': -4.9980003999466724, 'action_x_dist': 4.720137414589178, 'reward_count': 1.0, 'x_penalty': 3.8}\n",
      "{'counter': 106, 'ratio': 0.955, 'base_reward': 40.0, 'max_height': 3.718281828459045, 'floor_space': -4.959663583027856, 'action_x_dist': 3.488381630355155, 'reward_count': 1.0, 'x_penalty': 2.0}\n",
      "{'counter': 107, 'ratio': 0.96, 'base_reward': 47.0, 'max_height': 3.718281828459045, 'floor_space': -4.975559927079149, 'action_x_dist': 2.304901431054168, 'reward_count': 1.0, 'x_penalty': 0.6}\n",
      "{'counter': 108, 'ratio': 0.965, 'base_reward': 43.0, 'max_height': 3.718281828459045, 'floor_space': -4.916210019596277, 'action_x_dist': 2.977362711196349, 'reward_count': 1.0, 'x_penalty': 1.4000000000000001}\n",
      "{'counter': 109, 'ratio': 0.9675, 'base_reward': 48.0, 'max_height': 3.718281828459045, 'floor_space': -4.928515920612215, 'action_x_dist': 2.144781993395365, 'reward_count': 1.0, 'x_penalty': 0.4}\n",
      "{'counter': 110, 'ratio': 0.97, 'base_reward': 35.0, 'max_height': 3.718281828459045, 'floor_space': -4.9955020243926365, 'action_x_dist': 4.260718944831057, 'reward_count': 1.0, 'x_penalty': 3.0}\n",
      "{'counter': 111, 'ratio': 0.975, 'base_reward': 41.0, 'max_height': 3.718281828459045, 'floor_space': -4.959663583027856, 'action_x_dist': 3.3195788166773674, 'reward_count': 1.0, 'x_penalty': 1.7999999999999998}\n",
      "{'counter': 112, 'ratio': 0.9775, 'base_reward': 38.0, 'max_height': 3.718281828459045, 'floor_space': -4.9980003999466724, 'action_x_dist': 3.8153710180066804, 'reward_count': 1.0, 'x_penalty': 2.4}\n",
      "{'counter': 113, 'ratio': 0.9825, 'base_reward': 50.0, 'max_height': 3.718281828459045, 'floor_space': -4.982032361154967, 'action_x_dist': 1.8393972058572117, 'reward_count': 1.0, 'x_penalty': 0.0}\n",
      "{'counter': 114, 'ratio': 0.985, 'base_reward': 44.0, 'max_height': 3.718281828459045, 'floor_space': -4.9875156119873, 'action_x_dist': 2.8062186822161745, 'reward_count': 1.0, 'x_penalty': 1.2}\n",
      "{'counter': 115, 'ratio': 0.9875, 'base_reward': 41.0, 'max_height': 3.718281828459045, 'floor_space': -4.992006396588032, 'action_x_dist': 3.3195788166773674, 'reward_count': 1.0, 'x_penalty': 1.7999999999999998}\n",
      "{'counter': 116, 'ratio': 0.99, 'base_reward': 34.0, 'max_height': 3.718281828459045, 'floor_space': -4.999500024999167, 'action_x_dist': 4.392233696749656, 'reward_count': 1.0, 'x_penalty': 3.2}\n",
      "{'counter': 117, 'ratio': 0.995, 'base_reward': 28.0, 'max_height': 3.718281828459045, 'floor_space': -4.999500024999167, 'action_x_dist': 4.928515920612215, 'reward_count': 1.0, 'x_penalty': 4.4}\n",
      "{'counter': 118, 'ratio': 1.0, 'base_reward': 45.0, 'max_height': 3.718281828459045, 'floor_space': -5.0, 'action_x_dist': 2.636462120215243, 'reward_count': 1.0, 'x_penalty': 1.0}\n",
      "{'counter': 118, 'ratio': 1.0, 'base_reward': -200, 'max_height': 3.718281828459045, 'floor_space': -5.0, 'action_x_dist': 1.8393972058572117, 'reward_count': 1.0, 'x_penalty': 0.0}\n"
     ]
    }
   ],
   "source": [
    "base_env = gym.make('BinPack3D-v1', \n",
    "             container_size = (25, 4, 4), #(9, 11, 13),\n",
    "                boxSeqGenerator='CUT-2', \n",
    "                enabled_rotations = [Rotate.NOOP],\n",
    "                n_foreseeable_box = 3,\n",
    "                minSideLen = 1, # 2\n",
    "                maxSideLen = 2, # 5\n",
    "            )\n",
    "wrapped_env = RewardWrapper(base_env)\n",
    "\n",
    "\n",
    "frames = []\n",
    "obs = wrapped_env.reset()\n",
    "frame = wrapped_env.render()\n",
    "wrapped_env_rewards = []\n",
    "while True:\n",
    "    frames.append(frame)\n",
    "    box = wrapped_env.boxSeqGenerator.next_N_boxes()[0]\n",
    "    pos = (box.x,box.y)\n",
    "    act_pos = wrapped_env.position_to_actionIdx(pos)\n",
    "    rot = Rotate.NOOP\n",
    "    action = (act_pos, rot)\n",
    "\n",
    "    obs, reward, done,_, info = wrapped_env.step(action)\n",
    "    frame = wrapped_env.render()\n",
    "    print(info)\n",
    "    # print(reward,done,info)\n",
    "    wrapped_env_rewards.append(reward + wrapped_env_rewards[-1] if len(wrapped_env_rewards) > 0 else reward)\n",
    "    \n",
    "    if done: break\n",
    "\n",
    "wrapped_env.render()\n",
    "# print(obs)\n",
    "imageio.mimsave(GIF_DIR+\"/modified.gif\", frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#clear the plots\n",
    "plt.close(\"all\")\n",
    "#plot the rewards\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(rewards, label=\"baseline\")\n",
    "plt.plot(wrapped_env_rewards, label=\"modified_reward\")\n",
    "plt.plot(PPO_rewards, label=\"PPO\")\n",
    "plt.plot(A2C_rewards, label=\"A2C\")\n",
    "#limit the range of the x-axis to the number of steps of A2C\n",
    "# plt.xlim(0, max(len(PPO_rewards), len(A2C_rewards)))\n",
    "#scale the y-axis to the range of the rewards\n",
    "# plt.ylim(-4, 50)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flatten the observation space\n",
    "from gymnasium import ObservationWrapper\n",
    "import numpy as np\n",
    "\n",
    "class FlattenObservation(ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        \n",
    "    def observation(self, obs):\n",
    "        return np.array(obs).flatten()\n",
    "    \n",
    "base_env = gym.make('BinPack3D-v1',\n",
    "                container_size = (9, 11, 13),\n",
    "                    boxSeqGenerator='CUT-2', \n",
    "                    enabled_rotations = [Rotate.NOOP],\n",
    "                    n_foreseeable_box = 3,\n",
    "                    minSideLen = 2,\n",
    "                    maxSideLen = 5,\n",
    "                )\n",
    "wrapped_env = FlattenObservation(base_env)\n",
    "\n",
    "#we want to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implement a random agent to test the environment\n",
    "from gymnasium import Wrapper\n",
    "from gymnasium.spaces import MultiDiscrete\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class RandomAgent(Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.Prob = self.calculate_Prob()\n",
    "        \n",
    "    def step(self, action):\n",
    "        obs, reward, done, truncated, info = self.env.step(action)\n",
    "        return obs, reward, done, truncated, info\n",
    "    \n",
    "    def act(self, obs):\n",
    "        #choose a random position from the action space. action space is a multi-discrete space\n",
    "        (x, y) = self.env.action_space.sample()\n",
    "        print(self.env.action_space, x, y, self.env.container_size)\n",
    "        pos = x\n",
    "        rot = random.choice(self.env.enabled_rotations)\n",
    "        return (pos, rot)\n",
    "    \n",
    "    def create_agent(self, **kwargs):\n",
    "        agent = ValueIterationAgent(env = self.env, gamma=0.9, theta=0.0001, max_iter=1000)\n",
    "        agent.initialize()\n",
    "\n",
    "        return agent\n",
    "    \n",
    "    def calculate_Prob(self):\n",
    "        #calculate the transition probabilities\n",
    "        Prob = np.zeros((self.env.observation_space.nvec[0], self.env.action_space.n, self.env.observation_space.nvec[0]))\n",
    "        for s in range(self.env.observation_space.nvec[0]):\n",
    "            for a in range(self.env.action_space.n):\n",
    "                for s_prime in range(self.env.observation_space.nvec[0]):\n",
    "                    Prob[s, a, s_prime] = self.env.calculate_prob(s, a, s_prime)\n",
    "                    \n",
    "        return Prob\n",
    "\n",
    "class ValueIterationAgent(object):\n",
    "    def __init__(self, env=None, gamma=0.99, theta=0.001, max_iter=1000):\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.theta = theta\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "        self.disc_actions = self.env.action_space.nvec\n",
    "        self.disc_states = self.env.observation_space.nvec\n",
    "        self.Prob = self.env.Prob\n",
    "\n",
    "    def initialize(self):\n",
    "        self.value_policy, self.policy_function = self.value_iteration()\n",
    "\n",
    "    def value_iteration(self):\n",
    "        value_policy = np.zeros(self.disc_states)\n",
    "        policy_function = np.zeros(self.disc_states)\n",
    "\n",
    "        for i in range(self.max_iter):\n",
    "            delta = 0\n",
    "            for s in range(self.disc_states):\n",
    "                v = value_policy[s]\n",
    "                value_policy[s] = self.calculate_value(s, value_policy)\n",
    "                delta = max(delta, abs(v - value_policy[s]))\n",
    "\n",
    "            if delta < self.theta:\n",
    "                print('Converged at iteration', i)\n",
    "                break\n",
    "\n",
    "        for s in range(self.disc_states):\n",
    "            policy_function[s] = self.calculate_policy(s, value_policy)\n",
    "\n",
    "        return value_policy, policy_function\n",
    "    \n",
    "    def calculate_value(self, s, value_policy):\n",
    "        max_val = float('-inf')\n",
    "        for a in range(self.env.action_space.n):\n",
    "            val = 0\n",
    "            for s_prime in range(self.disc_states):\n",
    "                val += self.Prob[s, a, s_prime] * (self.env.R[s, a, s_prime] + self.gamma * value_policy[s_prime])\n",
    "            max_val = max(max_val, val)\n",
    "        return max_val\n",
    "    \n",
    "    def calculate_policy(self, s, value_policy):\n",
    "        max_val = float('-inf')\n",
    "        max_action = 0\n",
    "        for a in range(self.env.action_space.n):\n",
    "            val = 0\n",
    "            for s_prime in range(self.disc_states):\n",
    "                val += self.Prob[s, a, s_prime] * (self.env.R[s, a, s_prime] + self.gamma * value_policy[s_prime])\n",
    "            if val > max_val:\n",
    "                max_val = val\n",
    "                max_action = a\n",
    "        return max_action\n",
    "\n",
    "base_env = gym.make('BinPack3D-v1', \n",
    "                    container_size = (9, 11, 13),\n",
    "                    boxSeqGenerator='CUT-2', \n",
    "                    enabled_rotations = [Rotate.NOOP],\n",
    "                    n_foreseeable_box = 3,\n",
    "                    minSideLen = 2,\n",
    "                    maxSideLen = 5,\n",
    "                    )\n",
    "wrapped_env = RandomAgent(base_env)\n",
    "\n",
    "frames = []\n",
    "obs = wrapped_env.reset()\n",
    "frame = wrapped_env.render()\n",
    "rewards = []\n",
    "for i in range(100):\n",
    "    frames.append(frame)\n",
    "    action = wrapped_env.act(obs)\n",
    "    obs, reward, done,_, info = wrapped_env.step(action)\n",
    "    frame = wrapped_env.render()\n",
    "    print(reward,done,info)\n",
    "    rewards.append(reward)\n",
    "    \n",
    "\n",
    "wrapped_env.render()\n",
    "\n",
    "imageio.mimsave(DATA_DIR+\"/random.gif\", frames)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cs5446_ws]",
   "language": "python",
   "name": "conda-env-cs5446_ws-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
